{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 6965386\n",
      "Total Computations: 176223232\n",
      "FlexibleCNN(\n",
      "  (blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=12544, out_features=512, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        conv_filters=[32, 64, 128, 128, 256],\n",
    "        kernel_sizes=[3, 3, 3, 3, 3],\n",
    "        activation=\"relu\",\n",
    "        dense_neurons=512,\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Store configuration parameters\n",
    "        self.conv_filters = conv_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.dense_neurons = dense_neurons\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.dropout = dropout\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        \n",
    "        # Create model components\n",
    "        self._build_conv_blocks(activation)\n",
    "        self._build_classifier(conv_filters[-1] * 7 * 7)\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        \"\"\"Helper method to get activation function by name\"\"\"\n",
    "        activation_map = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"silu\": nn.SiLU(),\n",
    "            \"mish\": nn.Mish(),\n",
    "        }\n",
    "        return activation_map.get(name, nn.ReLU())\n",
    "        \n",
    "    def _build_conv_blocks(self, activation):\n",
    "        \"\"\"Build the convolutional part of the network\"\"\"\n",
    "        self.blocks = nn.ModuleList()\n",
    "        input_channels = self.in_channels\n",
    "        \n",
    "        # Create each convolutional block\n",
    "        for i, (filters, kernel_size) in enumerate(zip(self.conv_filters, self.kernel_sizes)):\n",
    "            # Use OrderedDict to maintain layer order\n",
    "            block_components = OrderedDict()\n",
    "            \n",
    "            # Add convolutional layer\n",
    "            block_components[f'conv{i}'] = nn.Conv2d(\n",
    "                input_channels, filters, \n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2\n",
    "            )\n",
    "            \n",
    "            # Add batch normalization if specified\n",
    "            if self.use_batchnorm:\n",
    "                block_components[f'bn{i}'] = nn.BatchNorm2d(filters)\n",
    "                \n",
    "            # Add activation function\n",
    "            block_components[f'act{i}'] = self._get_activation(activation)\n",
    "            \n",
    "            # Add pooling layer\n",
    "            block_components[f'pool{i}'] = nn.MaxPool2d(2)\n",
    "            \n",
    "            # Add dropout if specified\n",
    "            if self.dropout > 0:\n",
    "                block_components[f'drop{i}'] = nn.Dropout(self.dropout)\n",
    "            \n",
    "            # Create the sequential block and add it\n",
    "            self.blocks.append(nn.Sequential(block_components))\n",
    "            input_channels = filters\n",
    "    \n",
    "    def _build_classifier(self, flattened_size):\n",
    "        \"\"\"Build the classifier part of the network\"\"\"\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(flattened_size, self.dense_neurons)),\n",
    "            ('relu', nn.ReLU()),\n",
    "            ('dropout', nn.Dropout(self.dropout) if self.dropout > 0 else nn.Identity()),\n",
    "            ('fc2', nn.Linear(self.dense_neurons, self.num_classes))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def calculate_parameters_and_computations(self):\n",
    "        \"\"\"Calculate model parameters and computational complexity\"\"\"\n",
    "        params_per_layer = []\n",
    "        computations_per_layer = []\n",
    "        input_channels = self.in_channels\n",
    "        \n",
    "        # Calculate for each convolutional layer\n",
    "        for i, filters in enumerate(self.conv_filters):\n",
    "            kernel_size = self.kernel_sizes[i]\n",
    "            # Parameter count calculation\n",
    "            param_count = filters * (kernel_size**2 * input_channels + 1)\n",
    "            \n",
    "            # Add batch norm parameters if used\n",
    "            if self.use_batchnorm:\n",
    "                param_count += 2 * filters  # gamma and beta parameters\n",
    "            \n",
    "            # Calculate computational complexity\n",
    "            output_dim = 224 // (2**(i+1))\n",
    "            output_size = output_dim**2\n",
    "            comp_count = filters * kernel_size**2 * input_channels * output_size\n",
    "            \n",
    "            params_per_layer.append(param_count)\n",
    "            computations_per_layer.append(comp_count)\n",
    "            input_channels = filters\n",
    "        \n",
    "        # Calculate dense layer parameters and computations\n",
    "        last_channels = self.conv_filters[-1]\n",
    "        dense_param_count = (self.dense_neurons * last_channels * 7 * 7 + \n",
    "                            self.dense_neurons +\n",
    "                            self.num_classes * self.dense_neurons + \n",
    "                            self.num_classes)\n",
    "        \n",
    "        dense_comp_count = (self.dense_neurons * last_channels * 7 * 7 + \n",
    "                           self.num_classes * self.dense_neurons)\n",
    "        \n",
    "        return {\n",
    "            \"conv_params\": sum(params_per_layer),\n",
    "            \"dense_params\": dense_param_count,\n",
    "            \"conv_computations\": sum(computations_per_layer),\n",
    "            \"dense_computations\": dense_comp_count,\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = FlexibleCNN(\n",
    "        conv_filters=[32, 64, 128, 128, 256],\n",
    "        kernel_sizes=[3, 3, 3, 3, 3],\n",
    "        dropout=0.2,\n",
    "        use_batchnorm=True\n",
    "    )\n",
    "    results = model.calculate_parameters_and_computations()\n",
    "    print(\"Total Parameters:\", results[\"conv_params\"] + results[\"dense_params\"])\n",
    "    print(\"Total Computations:\", results[\"conv_computations\"] + results[\"dense_computations\"])\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c8dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6e9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fcce0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
